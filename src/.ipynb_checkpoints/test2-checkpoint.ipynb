{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import FreqDist\n",
    "\n",
    "\n",
    "import string\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from keras.preprocessing.text import one_hot\n",
    "\n",
    "\n",
    "PATH = \"/home/youcs/Documents/git/Sentiment_Test/data/Movie_Review/\"\n",
    "TRAIN = \"train.tsv\"\n",
    "\n",
    "############### Helper functions ###############\n",
    "\n",
    "def load_data(path,file):\n",
    "    try:\n",
    "        df = pd.read_csv(path+file)\n",
    "    except:\n",
    "        df = pd.read_table(path+file)\n",
    "    return df\n",
    "\n",
    "def tokenize2phrases(phrases):\n",
    "    tokens = [text_to_word_sequence(x) for x in df.Phrase]\n",
    "    return tokens\n",
    "\n",
    "def tokenize2unique(phrases):\n",
    "    tokens = [text_to_word_sequence(x) for x in df.Phrase]\n",
    "    tokens = [i for x in tokens for i in x]\n",
    "    return set(tokens)\n",
    "\n",
    "\n",
    "\n",
    "def lemmatize_tokenized_phrases(tokenized_phrases):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemm_token_phrases = []\n",
    "    for tokens in tokenized_phrases:\n",
    "        tokenize = []\n",
    "        for token in pos_tag(tokens):\n",
    "            pos = 'a'\n",
    "            if token[1].startswith('VB'):\n",
    "                pos = 'v'\n",
    "            elif token[1].startswith('NN'):\n",
    "                pos = 'n'\n",
    "            tokenize.append(lemmatizer.lemmatize(token[0], pos))\n",
    "        lemm_token_phrases.append(tokenize)\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    return [x for x in tokens if x not in stopwords.words('english')]\n",
    "\n",
    "\n",
    "'''\n",
    "0 - negative\n",
    "1 - somewhat negative\n",
    "2 - neutral\n",
    "3 - somewhat positive\n",
    "4 - positive\n",
    "plot count\n",
    "'''\n",
    "\n",
    "# sns.set(style='darkgrid')\n",
    "# ax = sns.countplot(x=\"Sentiment\", data=df)\n",
    "\n",
    "df = load_data(PATH,TRAIN)\n",
    "unique_tokens = tokenize2unique(df.Phrase)\n",
    "tokenized_phrases = tokenize2phrases(df.Phrase)\n",
    "lemmed_phrases = lemmatize_tokenized_phrases(tokenized_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_doc = [one_hot(d, len(unique_tokens)) for d in df.Phrase]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_doc = pad_sequences(encoded_doc,maxlen=max(map(len,encoded_doc)),padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import MaxPooling1D\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.models import Word2Vec\n",
    "# w2v = Word2Vec(lemm_token_phrases[:10],workers=8,min_count=1)\n",
    "# print(w2v)\n",
    "# print(list(w2v.wv.vocab))\n",
    "# print(w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=, output_dim=, input_length=))\n",
    "model.add(Conv1D(filters=, kernel_size=, ))\n",
    "model.add(MaxPool1D(pool_size=))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=))\n",
    "model.add(Dense(units=))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
